---
title: "Random forest"
author: "Josh Marvald"
date: "9/5/2020"
output: html_document
---
```{r}
library(tidyverse)
library(randomForest)
library(caret)
library(palmerpenguins)
```

*elo info*
<http://sleepomeno.github.io/blog/2015/09/08/Historical-ELO-Tennis-Rating/>

<https://www.ultimatetennisstatistics.com/eloRatings>

<https://fivethirtyeight.com/features/serena-williams-and-the-difference-between-all-time-great-and-greatest-of-all-time/#fn-2>










```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Set random seed to make results reproducible:
set.seed(17)
# Calculate the size of each of the data sets:
data_set_size <- floor(nrow(iris)/2)
# Generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(iris), size = data_set_size)
# Assign the data to the correct sets
training <- iris[indexes,]
validation1 <- iris[-indexes,]
```

```{r}
rf_classifier = randomForest(Species ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_classifier
varImpPlot(rf_classifier)
```

```{r}
# Validation set assessment #1: looking at confusion matrix
prediction_for_table <- predict(rf_classifier,validation1[,-5])
table(observed=validation1[,5],predicted=prediction_for_table)
```

```{r}
# Validation set assessment #2: ROC curves and AUC
# Needs to import ROCR package for ROC curve plotting:
library(ROCR)
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rf_classifier,validation1[,-5],type="prob")
# Use pretty colours:
pretty_colours <- c("#F8766D","#00BA38","#619CFF")
# Specify the different classes 
classes <- levels(validation1$Species)
# For each class
for (i in 1:3)
{
 # Define which observations belong to class[i]
 true_values <- ifelse(validation1[,5]==classes[i],1,0)
 # Assess the performance of classifier for class[i]
 pred <- prediction(prediction_for_roc_curve[,i],true_values)
 perf <- performance(pred, "tpr", "fpr")
 if (i==1)
 {
     plot(perf,main="ROC Curve",col=pretty_colours[i]) 
 }
 else
 {
     plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE) 
 }
 # Calculate the AUC and print it to screen
 auc.perf <- performance(pred, measure = "auc")
 print(auc.perf@y.values)
}
```


# New example

```{r}
head(penguins)
```


```{r}
set.seed(7)
data <- penguins %>%
        filter(is.na(bill_length_mm) == F & is.na(bill_depth_mm) == F & is.na(flipper_length_mm) == F & is.na(body_mass_g) == F)
               
data_size <- floor(nrow(data)/2)
# Generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(data), size = data_size)
# Assign the data to the correct sets
train <- data[indexes,] 
test <- data[-indexes,] 
```


```{r}
rf_mod <- randomForest(species ~ island, data = train, ntree = 100, mtry = 2, importance = T)
rf_mod

# not sure why this isnt working 
varImpPlot(rf_mod)
```

```{r}
# Validation set assessment #1: looking at confusion matrix
predictions <- predict(rf_mod, test[-1])

table(observed = test[1], predicted = predictions)
```




