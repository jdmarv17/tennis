---
title: "Exploration_process"
author: "Josh Marvald"
date: "6/5/2020"
output: html_document
---



  To begin my project, I started by exploring both match and point data. Before going into the exploration process it is important to understand the data involved. The first of the three data sets involved is match data from the past decade of Grand Slam matches for the ATP and WTA. The match data includes summary statistics from every Grand Slam match. The statistics most useful for this project are first and second serve percentages and the ranking of each player in the match. The second data set utilized in this project is point-by-point data from 2016 and 2017 Grand Slam matches. This data has information on each point played in every Grand Slam from 2016 and 2017. Some of the variables include serve speed, serve placement, rally count, and the current score of the point. The last data set used is a data set of point importance for each possible score in tennis. Importance values for a point give the average change in the probability of the winner of the point winning the match. Point importance will play an important role by being used as a predictor of different serve measurements in one of the models I will build for the project. During the data manipulation/cleaning period the importance data set was merged by score with the Grand Slam point-by-point data to assign importance values to each point played in Grand Slams.
  
  Exploring this data consisted of looking for any trends in the data. Specifically, any trends that were related to the serve in some way were of particular interest. To find these trends I had to manipulate the data sets into more usable forms. For example, the match data was originally in a form where each player in the match was listed in each row. To make it easier to look for trends in the data it was necessary to pivot the data into long form where only the winner or loser is listed. While this esentially doubled the matches in the data set, it was only used to look for trends. The model we will build for the match data uses the shorter form of the data.
  
  Once some manipulation had been done with the match data we were able to look for any relationships between first and second serve and the probability of a given player winning a match. Surprisingly we found patterns that indicated that for some players, higher first serve percentages corresponded with lower chances of winning a match.
  
```{r, echo=FALSE}
ggplot(matches_to_facet2, aes(x = first_serve, y = win)) +
  geom_jitter(height = 0.12, alpha = 0.15) +
  stat_smooth(method = "glm", method.args = c("binomial")) +
  facet_wrap("player")
```
  
  At this point we realized that there were other factors in play that must be contributing to this trend. Given a single match it doesn't make much sense that a higher first serve percentage would decrease a player's chance of winning. Our first theory was that it was possible that players were serving differently against lower and higher ranked players. Specifically our theory was that they might serve faster and less acurately against lower ranked players and slower and more accurately against higher ranked players. This could account for having more matches won with lower first serve percentages and more matches lost with higher first serve percentages. To see if this was the case we made a categorical variable for the opponent's ranking.
  
```{r, echo=FALSE}
ggplot(matches_to_facet2, aes(x = first_serve, y = win, color = opponent_ranking)) +
  geom_jitter(height = 0.12, alpha = 0.15) +
  stat_smooth(method = "glm", method.args = c("binomial")) +
  facet_wrap("player")
```
  As we can see from the plots above, this did not help simplify the matter. Different players have different trends and most players don't have the same trend for each category of opponent ranking. Seeing that looking at opponent ranking wasn't enough we realized that a more opponent specific model would be needed. This is what led us to decide on a Bradley-Terry model. This type of model takes a given player and looks at the matches they played against all opponents and returns different lines for different player match ups. This model allows for possibilitIES that less specific models would NOT reveal. Specifically, it is possible for a given player to have an overall negative relationship between their chance of winning a match and first serve percentage while still having a positive relationship between chance of winning and first serve percent against individual opponents. While this may not be the case for all players, it is a possibility that may not be accounted for with other types of models.
  
  Moving on to the point data, the exploration process was not as involved. The only relationship we were looking was point importance vs serve speed and serve placement. The more time consuming process turned out to be manipulating the importance data and the point-by-point data to merge correctly. The importance data set was calculated using the assumption that a tiebreaker would be played at 6-6 in a fifth set; however, in some grandslam tournaments no tiebreaker is played for the fifth set. Instead of a tiebreaker, play continues until one player leads the other by two games. Since this was the case, upon merging the point-by-point with the importance data there were points in some matches that were not assigned importance values due to the fact that no fifth set tiebreaker was played. To account for this we created a grouping variable that accounted for the difference in players' game scores in the fifth set once the score of 6-6 was reached. Once this was done we grouped the merged data by point score and the grouping variable and filled in the missing importance values based on the groupings.
  
  At this point it is important to understand how point importance is calculated. ________ started by looking at a large data set of points from ATP and WTA matches. They then took each occurence of a particular point and calculated the average change in the probability that the winner of the point wins the match. For example, if a point has an importance value of 0.05, then on average, the winner of that point sees an increase of 5% to the probability that they win the match. Obviously this isn't an exact change in match win probability for matches that reach that particular point but the original data set was large enough that the change in match win probabilty is approximately 5%.
  
  Following the data exploration and manipulation process, we moved on to building models. First we started by building a Bradley-Terry model for the match data. Before building the model we needeed to decide how to split the data into training and testing data sets. Since the match data covers 11 years of grand slam matches, there are some players that only appear in the beginning years and some players that appear in the later years. This introduced a possible complication. If a player wasn't in the training data set at all then it would be difficult for the model to predict accurate match win probabilites. To account for this complication we decided on doing a k-fold cross validation process. This process involves setting aside a single year of matches for our testing data set and uses the other 10 years for our training data set. We then build the model with the training data set and test its predictive ability by comparing its predictions against the actual outcomes from the testing data set. This process is then repeated 10 more times using each year as a testing data set with the other years as the training data set. This results in 11 different iterations of a Bradley-Terry model with 11 different sets of predictions.
  
  Seeing as our model returns a probability it is difficult to directly test the model predictions. We know who won in each match but we never know the true match win probability for a player in a match. To deal with this problem we used a calibration test to check the predicted probabilites. First we manipulated the data into a data frame with three columns. The first lists individual players, the second lists predicted match win probabilites, and the third lists whether that player won the match or not. At this point we had two entries for each match since there was only one player column. To get the data frame back to the appropriate size we filtered out all entries with predicted match win probabilities below 0.5. We knew that since there were two entries for each match, if one player in the match had a predicted match win proability below 0.5 then the other player must have a predicted match win probability above 0.5. This brought our row count to match the number of matches in our test data set. Once this was done we were able to complete the calibration test. The calibration test grouped the data into bins with size equal to 0.1 in predicted match win probability. For example one of the bins included all entries with predicted match win probabilities of 0.5 - 0.6. For this example if our model predicts a group of players to have match win probabilities of 0.5 - 0.6 then we would expect that 50-60% of those players would win their match. This is what we then checked for our calibration test for each iteration of the Bradley-Terry model. Upon completion we had 11 tables with proportions of matches won for each range of predicted match win probability.
  
  As we suspected, the models that used years nearer the center of the year range for the testing data set seemed to perform better than those with testing data sets from years closer to 2010 or 2020. Overall though, our models seemed to perform reasonably well. Not all of the proportions for matches won matched with the ranges for predicted match win probability but most were close or slightly below. This indicated to us that our model was performing as we hoped. In our final model that is used for the app we will use all 11 years to build the model but this method allowed us to check the performance of our model before building the final model.
  
  


