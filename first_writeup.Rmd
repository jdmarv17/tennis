---
title: "Exploration_process"
author: "Josh Marvald"
date: "6/5/2020"
output: html_document
---

```{r}
library(tidyverse)
source("data_cleaning.R")
source("merging_matches.R")
```

<!-- In order to knit this file, need to include all R code relevant to the graphics you included. This includes loading libraries and sourcing any data prep code. An option to avoid re-running all of the data prep steps is to write the final output data set to a .csv file (write.csv()). Then, instead of source("data_cleaning.R"), you'd read in that csv file with read_csv(). This could make things quicker if it's taking too long to source data_cleaning.  -->

  To begin my project, I started by exploring both match and point data. Before going into the exploration process it is important to understand the data involved. The first of the three data sets involved is match data from the past decade of Grand Slam matches for the ATP and WTA. The match data includes summary statistics from every Grand Slam match. The statistics most useful for this project are first and second serve percentages and the ranking of each player in the match. The second data set utilized in this project is point-by-point data from 2016 and 2017 Grand Slam matches. This data has information on each point played in every Grand Slam from 2016 and 2017. Some of the variables include serve speed, serve placement, rally count, and the current score of the point. The last data set used is a data set of point importance for each possible score in tennis. Importance values for a point give the average change in the probability of the winner of the point winning the match. Point importance will play an important role by being used as a predictor of different serve measurements in one of the models I will build for the project. During the data manipulation/cleaning period the importance data set was merged by score with the Grand Slam point-by-point data to assign importance values to each point played in Grand Slams.
  
  Exploring this data consisted of looking for any trends in the data. Specifically, any trends that were related to the serve in some way were of particular interest. To find these trends I had to manipulate the data sets into more usable forms. For example, the match data was originally in a form where each player in the match was listed in each row. To make it easier to look for trends in the data it was necessary to pivot the data into long form where only the winner or loser is listed. While this esentially doubled the matches in the data set, it was only used to look for trends. The model we will build for the match data uses the shorter form of the data.
  
  Once some manipulation had been done with the match data we were able to look for any relationships between first and second serve and the probability of a given player winning a match. Surprisingly we found patterns that indicated that for some players, higher first serve percentages corresponded with lower chances of winning a match.
  
```{r, echo=FALSE}
ggplot(matches_to_facet2, aes(x = first_serve, y = win)) +
  geom_jitter(height = 0.12, alpha = 0.15) +
  stat_smooth(method = "glm", method.args = c("binomial")) +
  facet_wrap("player")
```
  
  At this point we realized that there were other factors in play that must be contributing to this trend. Given a single match it doesn't make much sense that a higher first serve percentage would decrease a player's chance of winning. Our first theory was that it was possible that players were serving differently against lower and higher ranked players. Specifically our theory was that they might serve faster and less acurately against lower ranked players and slower and more accurately against higher ranked players. This could account for having more matches won with lower first serve percentages and more matches lost with higher first serve percentages. To see if this was the case we made a categorical variable for the opponent's ranking.
  
```{r, echo=FALSE}
ggplot(matches_to_facet2, aes(x = first_serve, y = win, color = opponent_ranking)) +
  geom_jitter(height = 0.12, alpha = 0.15) +
  stat_smooth(method = "glm", method.args = c("binomial")) +
  facet_wrap("player")
```
  
  As we can see from the plots above, this did not help simplify the matter. Different players have different trends and most players don't have the same trend for each category of opponent ranking. Seeing that looking at opponent ranking wasn't enough we realized that a more opponent specific model would be needed. This is what led us to decide on a Bradley-Terry model. This type of model takes a given player and looks at the matches they played against all opponents and returns different lines for different player match ups. Bradley-Terry models do this by returning an intercept and slope for each player for a given predictor. With these parameters and a given value for the predictor a player's "ability" can be calculated for each player involved in a match. The two players' abilities are then backtransformed using a logit formula to return a predicted probability that one player will win the match given that value of the predictor.
  
  This type of model allows for possibilities that less specific models would not reveal. Specifically, it is possible for a given player to have an overall negative relationship between their chance of winning a match and first serve percentage for all opponents while still having a positive relationship between chance of winning and first serve percent against individual opponents. While this may not be the case for all players, it is a possibility that may not be accounted for with other types of models.
  
  Moving on to the point data, the exploration process was not as involved. The only relationship we were looking was point importance vs serve speed and serve placement. The more time consuming process turned out to be manipulating the importance data and the point-by-point data to merge correctly. The importance data set was calculated using the assumption that a tiebreaker would be played at 6-6 in a fifth set; however, in some grandslam tournaments no tiebreaker is played for the fifth set. Instead of a tiebreaker, play continues until one player leads the other by two games. Since this was the case, upon merging the point-by-point with the importance data there were points in some matches that were not assigned importance values due to the fact that no fifth set tiebreaker was played. To account for this we created a grouping variable that accounted for the difference in players' game scores in the fifth set once the score of 6-6 was reached. Once this was done we grouped the merged data by point score and the grouping variable and filled in the missing importance values based on the groupings.
  
  At this point it is important to understand how point importance is calculated. ___________ started by looking at a large data set of points from ATP and WTA matches. They then took each occurence of a particular point and calculated the average difference between the probability that the winner of the point wins the match and the probability that that same player wins the match if they lose the point. For example, if a point has an importance value of 0.05, then on average, the point winner's probability of winning the match is 5% higher than if they lost the point. Obviously this isn't an exact difference in match win probability for matches that reach that particular point but the original data set was large enough that the change in match win probabilty is approximately 5%.
  
  Following the data exploration and manipulation process, we moved on to building models. First we started by building a Bradley-Terry model for the match data. Before building the model we needeed to decide how to split the data into training and testing data sets. Since the match data covers 11 years of grand slam matches, there are some players that only appear in the beginning years and some players that appear in the later years. This introduced a possible complication. If a player wasn't in the training data set at all then it would be difficult for the model to predict accurate match win probabilites. To account for this complication we decided on doing a k-fold cross validation process. This process involves setting aside a single year of matches for our testing data set and uses the other 10 years for our training data set. We then build the model with the training data set and test its predictive ability by comparing its predictions against the actual outcomes from the testing data set. This process is then repeated 10 more times using each year as a testing data set with the other years as the training data set. This results in 11 different iterations of a Bradley-Terry model with 11 different sets of predictions.
  
  Seeing as our model returns a probability it is difficult to directly test the model predictions. We know who won in each match but we never know the true match win probability for a player in a match. To deal with this problem we used a calibration test to check the predicted probabilites. First we manipulated the data into a data frame with three columns. The first lists individual players, the second lists predicted match win probabilites, and the third lists whether that player won the match or not. At this point we had two entries for each match since there was only one player column. To get the data frame back to the appropriate size we filtered out all entries with predicted match win probabilities below 0.5. We knew that since there were two entries for each match, if one player in the match had a predicted match win proability below 0.5 then the other player must have a predicted match win probability above 0.5. This brought our row count to match the number of matches in our test data set. Once this was done we were able to complete the calibration test. The calibration test grouped the data into bins with size equal to 0.1 in predicted match win probability. For example one of the bins included all entries with predicted match win probabilities of 0.5 - 0.6. For this example if our model predicts a group of players to have match win probabilities of 0.5 - 0.6 then we would expect that 50-60% of those players would win their match. This is what we then checked for our calibration test for each iteration of the Bradley-Terry model. Upon completion we had 11 tables with proportions of matches won for each range of predicted match win probability.
  
  As we suspected, the models that used years nearer the center of the year range for the testing data set seemed to perform better than those with testing data sets from years closer to 2010 or 2020. We also found that the Bradley-Terry model performs better for ATP matches than for WTA matches. We suspect that this could be due to several reasons. One reason is that WTA matches are best 2 out of 3 sets while ATP matches are best 3 out of 5 sets. With longer format matches, it is likely that upsets occur less frequently. If there are less upsets in ATP matches it makes sense that any model would be better at predicting ATP match winners than predicting WTA match winners. Another possible reason the Bradley-Terry model performs better for ATP matches is that the serve is more of a determining factor in men's tennis than women's tennis. It is possible that due to serves being faster in ATP matches they have a larger influence on match outcomes than in WTA matches. Overall though, our models seemed to perform reasonably well. Not all of the proportions for matches won matched with the ranges for predicted match win probability but most were close or slightly below. This indicated to us that our model was performing as we hoped. In our final model that is used for the app we will use all 11 years to build the model but this method allowed us to check the performance of our model before building the final model.
  
  For the point level data we initially decided to use a mixed effects model with the serving player and point importance as predictors for serve speed. This type of model is useful because it allows for fixed and random effects in the model. We built several different models of this type. The first model used just the serving player variable for a random effect with random intercepts. The second model used point importance for a fixed effect and serving player for a random effect with random intercepts. The last model we built used importance for a fixed effect and serving player for a random effect with random intercepts and slopes. By comparing these three models we hoped to be able to establish how important the point importance values were in predicting serve speed. We found that all three of these models performed very similarly. Most striking was the fact that while the importance predictor was significant, it barely improved serve speed prediction. Seeing as this was the case, we believe that point importance does influence serve speed, but not enough to warrant using it in a model.
  
  After finding out that point importance did not play a large role in influencing serve speed we decided to try building a neural network for serve prediction. Neural networks are systems of algorithms that are loosely based on the structure of neural networks in human brains and are able to improve themselves through an intensive training process. They are composed of layers of neurons that accept signals from preceding layers of neurons. The first layer is called the input layer which consists of one neuron for each predictor. The next layer is called the hidden layer and has the number of neurons set by the person building the network. Each of the neurons in the hidden layer receives a signal from each of the neurons in the input layer. The neurons in the hidden layer assign weights to the signals received before then sending a signal to the final layer which is the output layer. The output layer has one neuron if the variable being predicted is numerical or many neurons if the variable being predicted is categorical. At the output layer the neurons combine all the signals from each of the hidden layer neurons to calculate a final prediction.
  
  It is also important to understand how neural networks are trained. When a neural network runs training data for the first time it randomly assigns weights to all of the hidden layer neurons and all of the connections between neurons. The network then undergoes a process called backpropagation. Backpropagation consists of defining an error function that models the residuals of its predictions. After assigning random weights, the network computes partial derivatives of the error function with respect to each weight assigned to the network's neurons and connections. This is done in attempt to find a local minimum of the error function. At the error fucntion's minimum the partial derivatives with respect to the weights will be 0. Thus, after each iteration of the network assigning weights and evaluating the error function, the network re-assigns weights to the neurons to see if the partial derivatives got closer to 0. This is repeated until a certain threshold for the error function is reached or until a specified number of iterations is reached. Backpropagation is a powerful training method that can often result in better predictions than other types of models. One drawback that neural networks experience is a lack of interpretability. Since each predictor is sent to each of the hidden layer neurons it is difficult to say which predictors are contributing to the results the network gives. In the case of this project, a lack of interpretability is a signficant drawback. A large motivating factor of this portion of the project is a desire to see if point importance plays a role in serve speed and if so, how large a role. Neural networks make this nearly impossible.
  
  Our first neural network used point importance and serving player as predictors to predict serve speed. Unfortunately, this neural network was only marginally better at predicting serve speed than the mixed effects model we previously built. In addition, our neural network did not help us understand how point importance plays a role in serve speed. Due to this, we decided to build a different neural network that would predict serve location instead of serve speed. 
  
  This new neural network was different in that it was predicting a categorical variable rather than a numerical variable. It also included many more predictors than the previous network. These include a variable for whether the serve is a first or second serve, a variable for the side of the court being served on, a variable for whether the server is ahead in the game being played, a variable for the domninant hand of the serving player, a variable for the height of the returner, and a variable for whether the match is being played on hard court or a different surface. Like the previous neural network this new network also inluded a predictor for point importance and indicator variables for the serving player.
  
  Once our new neural network was trained we needed to decide how to evaluate its predictive abilities. Since the categorical response had three options we obviously needed the network to be able to predict serve location correctly more than one third of the time. We also wanted the network to predict serve location correctly more often than a basic proportion model. The proportion model we compared to looked at the training data set to see which serve location appeared most often and predicted this serve location every time for the test data set. Using this method we found that a basic proportion model would predict correctly between 45-47% of the time depending on the sampling for the training and test data sets. In comparison our neural network was able to predict serve location correctly between 50-52% of the time. 
  
  Considering that the test data set included 8000 points a difference of 5% in predictive abilities is significiant. Unfortunately, we came to the conclusion that this difference was not enough to include the neural network model in our app. While the netowrk is clearly better than random chance and a basic proportion model we felt that its predictive abilities were not strong enough to incorporate it into the app especially considering the loss in ability to interpret how the model was making the predictions it was returning. We expect to dig deeper into this throughout the coming school year.
  
  At this point we had completed our model building and testing and were ready to move onto constructing plots for our Bradley-Terry models and building the shiny app that would display these plots. First, we decided to construct a plot for a single player of interest. In order to do this we needed to use our model to calculate individual match win probabilites for a range of predictor values. We started by looking at Roger Federer with first serve percentage as our predictor. Federer had a first serve percentage range of 0.52 to 0.77. We decided to split this range 30 times to give us 30 first serve percentage values that we wouls use to calculate match win probabilities. Since Federer was the player of interest we averaged each  opponent's first serve percentages to give us values to calculate their abilities. 
  
  We already had an intercept and slope for each player so we used the 30 first serve percentage values to calculate Federer's ability at each first serve percentage value. To find his opponents' abilities we used their average first serve percentage to calculate an average ability for each oppoenent. Finally, looking at Federer's ability at each first serve percentage value and his opponents' average ability and backtransforming the difference in abilities we obtained predicted match win probabilites for Federer at each first serve percentage value. This gave us paired value points with a first serve percentage and a predicted match win probability for each of Federer's opponents.Using these points we could then create predicted match win probability lines for Federer against each opponent by connecting the points on a plot.
  
  Once we had completed one plot for Federer there was no need to create plots for all other players since we would use the code we had written in the app and replace Federer with a player of interest selected by the app user.
  
*21 wta matches dropped due to NAs*

