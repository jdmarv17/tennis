---
title: "Analyzing the Importance of the Serve in Professional Tennis"
author:
- Josh Marvald
- Dr. Matt Higham
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_document:
    toc: yes
    toc_depth: 4
  word_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

```


```{r, echo = FALSE}
library(tidyverse)
library(here)
```

## Abstract

The goal of this project is to predict match outcome of professional tennis matches using a random forest model for prediction with a large data set of Grand Slam matches for the ATP. While not an exact continuation, this project is related to my summer SLU Fellowship titled “Analyzing the Importance of the Serve in Professional Tennis.” These two projects are different in their purposes, but they share a common data set and much of the code was used for both.  

Upon completion of our project, we have built and tested random forest models that are reasonably accurate for prediction. The models take several variables that gauge the score and returns a prediction for match outcome at each point within a match. Since there are a large number of points played in each match, we can plot graphs depicting the change in predicted match outcome over the course of a match.

## Introduction 

Much of the inspiration for this project came from Lock & Nettleton (2014), which used a random forest to predict the outcome of an NFL game at each play. After building this model, the authors were able to plot win probability of specific matches against a given variable by holding the other variables constant. This gives the reader a sense of how the win probability changes with each variable. 
  
Our project attempted to do something analogous for tennis. While on the surface our project may appear quite similar to Lock and Nettleton’s, tennis introduces complications that are not present in football games. Specifically, Lock and Nettleton were able to take advantage of the fact that football games last a specific amount of time. This means that at each play in a game, there is a specific amount of time left to play. Since this is the case, some of the other variables included in the random forest become more meaningful over the course of a game. For example, consider a score of 21-7 in a football game. If this score occurs with 5 minutes left in the first quarter, it is an indication of which team is favored at that time, but it is not deterministic of the outcome. On the other hand, if this score occurs with 5 minutes left in the fourth quarter, it is much more probable than it was in the first quarter that the team with 21 points will win the game.
  
However, tennis has no equivalent variable. Tennis has no game clock that limits the length of matches. In fact, there have been tennis matches that have lasted over 10 hours. Since this is the case, improving the accuracy of predictions over the course of a tennis match is a slightly more difficult task. Much of the difficulty in the later stages of our project stemmed from this inherent difference between football and tennis.

## Random Forests

In order to explain what a random forest is, it is first important to introduce decision trees. A decision tree can be considered a set of rules that separates data into groups based on some variable. At the beginning, the first rule splits the data into two branches. Each of these branches then have another rule which splits the two branches into four. This process can be repeated until the data left on each branch are separated into unique groups. For example, we may have a data set with a response variable that is a categorical variable. We could specify a number variables for a decision tree to use to split the data so that the data points on a given branch are all of the same category. 
  
Once the decision tree has been trained, we can take new data and feed it into the tree. The tree will feed the data point into the correct branch based on the rules it established at each branch split. It will then return a prediction for the response variable based on value corresponding with the branch that the data point was led to by the rules of the tree.
	
As may be implied with its name, a random forest consists of a large number of decision trees that are random in two ways. First, a random sample with replacement from the training data set is used to train each tree. Sampling randomly for each tree can cut down on the random forest’s sensitivity to training data that might hinder its predictive capabilities with new data. Second, each decision tree randomly samples from the predictors and only uses a subset of the features specified. By having random predictors for each decision tree, the trees as a whole are less correlated which can further help the model be less sensitive to training data. Once a random forest is trained, new data can be introduced, and the model will return a category prediction for each data point.
	
For our project, we used a specific type of random forest called a regression random forest. Instead of returning a group prediction, a regression random forest returns a value. A regression random forest uses a quantitative variable for the response and returns the average of the response from the decision trees. In our case, since we were trying to predict match outcome, a zero corresponded with player 1 winning and a one corresponded with player 2 winning. Then for a single data point, each decision tree will give a prediction of zero or one and the forest as a whole returns a prediction that is the average of the ones and zeros. This value will always be between zero and one and will represent the probability that player 1 loses the match.

## Description of Data

The first data set we used had match-level data of Grand Slam matches for men’s ATP players from the years 2016-2018. This data set was sourced from Jeff Sackmann’s Github repository and contained statistics for matches as a whole (Sackmann, 2020). The second data set we used contained point-level data for the same matches included in the match-level data set. The point-level data set was also sourced from Jeff Sackmann’s Github and contained statistics for every point that occurred in every men’s Grand Slam match. 
  
The main reason we chose to focus on men’s Grand Slam matches is due to the difference in length of matches between men’s and women’s tennis. Men’s Grand Slam matches are best three out of five sets while women’s Grand Slam matches are best two out of three sets. With a longer format, we suspected that a random forest would be better able to differentiate situations that are favorable to winning a match from those that are unfavorable. In other words, since a random forest works by finding cutoffs in certain variables within the data, having a longer match would make those cutoffs easier to find.

## Model

Once we had prepared our data properly, we were able to begin building our random forest using the `randomForest` package in `R` (Liaw & Wiener, 2002).  That is, cutoffs in the ranking difference feature in the training data may not correspond with any meaningful cutoff in the test data. 
  
One of the in-match variables we used in our first random forest was the difference in game score within a set. If, for a given match and a given set, player 1 is leading, the difference in game scores will indicate how many games player 1 is ahead by in the set. If player 2 is leading the difference will be negative and the absolute value will indicate player 2’s lead in the set. This, along with other in-match variables that indicate score differences, would allow the random forest to find certain scores that are more favorable for one player over the other. 
	
Before training our random forest, we had to decide how many decision trees to include and how many variables for each tree to sample from the pool of features. We ended up finding that as the number of decision trees is increased, predictions become more accurate, but it seems to be a case of diminishing return. The increase in predictions’ accuracy as the number of trees increases starts to decrease in rate. Our first random forests used 50-100 trees. This range allowed for generally high accuracy with relatively low training time. Depending on how many features we would include a random forest with 100 trees would take several minutes for a laptop to train. The parameter to set how many features each tree will sample was more difficult to gauge. Including too few features and the predictions were not very accurate but we also saw that too many features was detrimental. Since decision trees are sensitive to training data, then having each tree sample most of the features would make the trees highly correlated. As a result, the random forest would be too sensitive to training data and would not predict accurately with new data.
	
After building and testing many random forests, the model we settled on included only in-match variables. These included the set difference, number of sets played, game difference, number of games played, point difference, number of points played, and difference in breaks in each set. A break in tennis is when a player wins a game when the opponent is serving. The serve is seen as an advantage in tennis so to break an opponent is a significant indicator for success in a given set. We found that pre-match variables like ranking difference were not appropriate to include in a random forest. Trends in pre-match variables in the training data set may not hold in the test data set. Because of this, the features we used attempted to gauge the progress in the match both in terms of the difference in performance between the two players, but also in terms of how much of the match has been played at each point. For example, since a game lead of three games is more important in the fifth set of a match than a three game advantage in the first set of a match, our random forest can account for this with the game difference feature and the total games played feature.
	
Our final random forest model also used 1500 decision trees and five features per tree. Using the high performance computer we were able train much larger random forest than we were able to on laptps. We found that while prediction accuracy increased as the number of trees increased, the rate of increase in prediction accuracy seemed to drop off considerably after the random forest has more than 1000 tree. Using five features per decision tree seemed to work well for our purposes. Including fewer features caused the prediction accuracy to drop simply because each tree had less information to work with. Including more features also caused the prediction accuracy to drop because the trees were too highly correlated. This made the random forest as a whole overfitted to the training data set and as a result its predictions were less accurate.


## Results

  The random forest we finished with was fairly accurate for prediction. Tested with approximately 300 matches and 65,000 points, our random forest predicts the outcome of the match correctly at about 75% of the points. To ensure that these results were meaningful we tested them against two metrics. First, our model is clearly better than random chance. If our random forest was predicting correctly less than 50% of the time our model would not be adding any value to match prediction. The second benchmark we checked was a simple model that always predicts the player with a higher ranking will win. A model of this type predicts correctly 65-70% of the time so we knew our results improved prediction.
  
  After our random forest was trained and tested we were able to plot the course of particular matches to see how our random forest's predictions changed as a result of real score changes in each of the matches. This can best be seen by looking at an example. Below is a plot depicting our random forest's match-outcome predictions over the course of a match played between Roger Federer and Juan Martin del Potro in the quarterfinals of the 2017 U.S. Open. This match was four sets long and each set is depicted with a different color on the graph. In this match, a one corresponds to del Potro winning the match and a zero corresponds to Federer winning the match.
  
  Del Potro ended up winning this match but we will examine the second set in particular. The first set was won by del Potro with a score of 7-5 and, as can be seen in the graph, our random forest is predicting with a high probability that del Potro will win the match at the end of the first set. As the second set starts, the first three games go fairly normally with Federer and del Potro winning a similar amount of points. Then, in the fourth game, starting at point number 90, del Potro is serving and Federer wins every point in the game. In doing this, Federer swings the point difference in his favor, increases his game lead in the set to two games, and gives him a one break lead in the set. 
  
  If we look at the graph we can see the random forest suddenly adjust its predictions in the second set. Up until point 90 the random forest is predicting del Potro to win but Federer is able to swing all of random forest's features in his favor except for set advantage. Obviously, a set advantage is more important than the other features so we see that the random forest is still predicting del Potro will win with 62% probability. Although this match was completed years ago, it is interesting to watch this game played while considering the path of the match-outcome probability plotted.
  
```{r}
knitr::include_graphics(here("SYE", "pics", "fed_delpo.png"))
```

## Conclusion and Future Work

In completing this project we hope to have demonstrated the viability of using random forest models for prediction in professional tennis. As sports become more and more data driven, the models that can be made to predict certain results in sports can be a valuable tool to inform fans, athletes, or commentators on a particular aspect of a given sport. In the case of our project, we believe that our random forest has provided a way for tennis fans to have a better sense of the progress of a professional tennis match. Since our random forest performs better than chance or simple proportion models, at any moment in a tennis match it can be used to gauge which player is favored at that point. Our random forest doesn't use any player specific features so it can be used with any professional tennis player at any Grand Slam tournament.
  
While we are happy with our results thus far, we see that there is still work to do. To improve our random forest's performance we suspect that more work could be done to perfect the features included. While the method we used of including variables that gauge score separation and cumulative scores allowed the random forest to find trends within the data, there could potentially be variables that can better appraise the progress of a match. In addition to which features to include, testing the parameter that sets how many features for each decision tree to subset could also improve the random forest's predictions. Checking the decision trees correlations could help in this way.
  
Another area where further work remains is in improving the model's predictions in the earlier stages of matches. Oftentimes in the first or second set it will predict a probability of very close to zero or one even though the outcome of the match is quite uncertain at those points. Finding new features that could help a random forest return less certain predictions earlier on in matches would be a valuable improvement.
  
Finally, we hope to add this random forest to a Shiny web-app we already created that allows users to analyze specific player match-ups with a Bradley-Terry model. Once added to our app, the user would be able to enter in a particular score and the player who served first in the match and the random forest would return a predicted probability for the outcome of the match. By doing this, tennis fans could use our app while watching a professional tennis match to stay updated on the predicted outcome of the match. We see one use of our app as being a way for tennis fans or commentators to more fully understand the progress of a match. While commentators are very knowledgeable about tennis, they are limited in the information they are able to process at any given moment. They are able to access a number of real-time statistics but no commentator can calculate the probability of a certain player winning. If a commentator had access to our random forest's predictions they would be more informed on who is favored to win at any given moment. This would both increase the commentators' knowledge of a given match and provide a more full experience for those fans viewing.

## Appendix

Our initial instinct was to find a mix of pre-match variables and in-match variables to include as features for our model. For example, one of the pre-match variables we used for our first random forest was the difference in ranking points between the two players in a given match. Professional tennis players earn ranking points for winning matches, but the ranking points expire every year. We reasoned that since ranking points are indicative of a player’s performance over the past year leading up to a match, including the difference in ranking points as a predictor would help the random forest make better predictions early on in matches where in-match variables are not yet influential. We eventually realized that using the difference in ranking points as a feature was not practical because the trends in the feature in the training data set will not necessarily correspond to any trend in the test data set. 

## Works Cited

A. Liaw and M. Wiener (2002). Classification and Regression by randomForest. R News 2(3), 18--22.

Lock, D., & Nettleton, D. (2014). Using random forests to estimate win probability before each play of an NFL game. Journal of Quantitative Analysis in Sports, 10(2), 197-205.

Sackmann, Jeff. tennis_slam_pointbypoint, (2020), GitHub Repository, https://github.com/JeffSackmann/tennis_slam_pointbypoint

